The program should do the following

	1. Read command line arguments: URL to download, an output file name, and whether the URL should be downloded recursively. That's it for now.
	2. Parse the URL so that we know the protocol, hostname, port, and path. That's it for now.
	3. Resolve the host.
	4. Connect to the host.
	5. Create an HTTP request to the host requesting the file specified by the path in the URL.
	6. Read the response header into a response structure.
	7. Determine the HTTP status code given by the server.
	8. If the server gives a 200, read the content into the output file (or standard output if no output file has been specified).
	9. If the server gives a 3xx, parse the response header and extract the 'Location' field, provided that one exists.
		- Repeat step 2. Later, we may try to keep the existing connection if we are connected to the same host/keep track of persistent
			connections (if the header of the response says that the connection is of type keep-alive, then we should recongnize that
			and reuse it when applicable)
		- If a maximum number of redirections is reached, stop trying to redirect (also, if we redirect to the exact same place, we 
			should just stop the loop there, because there is no reason to believe that the result will change)
	10. If the server gives a 400, exit with an appropriate error code. 


We are going to make the decision to write to the file as the server content is being read. That means we're going to have to locate the 
end of the response header aka the beginning of the actual content and then start writing to the output file but this shouldn't take too 
much. I'm not really sure what's going wrong with the reading of the pdf. Maybe the null-termination is causing a problem... let's
investigate.


Good news -- THE HASH TABLE ACTUALLY WORKS!!! (for the most part)

So this version is clearly superior to the previous one. It is much more clean.

The big TODOs:

	1. Parsing
	2. Link conversion to local file paths
	3. SSL support (with OpenSSL, no GNUTLS)

Unfortunately, I have a ton of schoolwork this week so I'm not sure how much I'm going to be able to carry this moment into the
week. Tomorrow I will try to get all of my homework done (I actually got a lot done that day but I still ended up slacking on Thursday 
and destroyed my 351 grade... not really I only lost a homework, each of which are only 1% of the total grade. But still) and then study 
for 6-7 hours for the exam over the next two days

but I should have some free time to work on this. This is why I don't have a social life -- otherwise there would be no way that I could
possibly get good grades and work on personal projects. Also, stop procrastinating internship applications. That's another thing I'm 
going to do tomorrow. Tomorrow is a no-BS work day. I will be attending classes and working quite literally the entire day with the 
occasionally break by transitioning locations. It's gonna be a lot of fun.

The reason we're still getting those segfaults is because we're loading the entire file into memory... don't do that.
That's going to be the next fix. Once that's taken care of, we should be able to download larger files than we are
currently capable of.

Now that I've successfully remembered what the code actually does, it's time to write a parser (THE FUN PART... RIGHT?).

Don't even refer to the previous parser... it sucks. Rewrite the parser (reinvent the wheel).
