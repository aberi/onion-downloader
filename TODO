1. It's time to create some sort of url queue struct so that we can enqueue every link that we find and then
   decide, based on the desired level of recursion, whether or not to try and download all the files on that
   queue.

2. So I have class in about ten minutes; it doesn't make a ton of sense to really start seriously working
   on the queue struct. You can fool around for a bit. I'll give you permission.

3. We still have to work on the parser, but it's working well enough for enough sites that
   I'm going to allow you to move forward for the time being

4. We also need to make it a lot faster, but I think that's a matter of finding
   what is probably a glaring error.
